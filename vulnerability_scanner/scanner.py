#!/usr/bin/env python
import re
import urllib.parse

import requests


class Scanner:
    def __init__(self, url):
        self.target_url = url
        self.target_links = []

    def extract_links_from(self, url):
        try:
            response = requests.get(url)
            if response and response.content:
                return re.findall('(?:href=")(.*?)"', response.content.decode())
        except Exception:
            pass

    def craw(self, url=None):
        if url is None:
            url = self.target_url
        href_links = self.extract_links_from(url)
        if href_links:
            for link in href_links:
                link = urllib.parse.urljoin(url, link)
                if "#" in link:
                    link = link.split("#")[0]
                if self.target_url in link and link not in self.target_links:
                    self.target_links.append(link)
                    print(link)
                    self.craw(link)
